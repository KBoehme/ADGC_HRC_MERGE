""" ADGC 2.0 - Relatedness analysis

 General overview of process
    1. Given raw genotyped snps for each study, create a list of common snps.
    2. LD-prune list of common snps.
    3. Extract these subset of snps from plink files.
    4. Run King-Robust on this LD-pruned, common dataset to generate list of related (within 3rd degree) individuals.
    6. Remove related individuals leaving one
    7. Run Eigenstrat on this LD-pruned, common, unrelated dataset to generate PC's
    8. Merge PC's with sample covariate files.


    snakemake -np --verbose -j 999 --cluster-config ../scripts/relatedness/cluster.json -s ../scripts/relatedness/Snakefile --cluster \
    "sbatch \
    --ntasks {cluster.ntasks} \
    --time {cluster.time} \
    --mem {cluster.mem} \
    --job-name {cluster.name} \
    --mail-type {cluster.mail-type} \
    --mail-user {cluster.mail-user} \
    --parsable" \
    --cluster-status ../scripts/slurm_status.py
"""

from pathlib import Path

# This is the shared folder location, however due to lack of space cant be used.
# ROOT = "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/"
ROOT = "/fslhome/fslcollab192/compute"
WORKING_DIR = "ADGC_HRC_COMBINE"
WORKFLOW_NAME = "relatedness_workflow"
# RESOURCES_DATA_FOLDER = "resources"
# SCRIPTS_DATA_FOLDER = "scripts/python_scripts"
BIM_DATA_SOURCE = os.path.join(ROOT, "ADGC_Data", "ADGC_Raw", "bims")
PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", WORKFLOW_NAME)

FINAL_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "final")
EXTRA_FOLDER = os.path.join(FINAL_DATA_SOURCE, "auxiliary")

# RESOURCES = os.path.join(ROOT, RESOURCES_DATA_FOLDER)
# SCRIPTS = os.path.join(ROOT, SCRIPTS_DATA_FOLDER)
LOGS = os.path.join(PROCESS_DATA_SOURCE, "logs")
PLINK_EXT = ["bed", "bim", "fam"]
KING_EXT = ["allsegs.txt", "unrelated_toberemoved.txt", "unrelated.txt"]

localrules: bim_to_snp_ids_only, get_common_snps, remap_fids_for_king

BIM_NAMES = [Path(name).stem for name in os.listdir(BIM_DATA_SOURCE)]

rule all:
    """ This is an artificial snakemake rule that basically specifies as input
    the final files im attempting to get out of the workflow."""
    input:
        expand(os.path.join(PROCESS_DATA_SOURCE, "king", "adgc_hrc_king{ext}"), ext=KING_EXT)

# rule eigenstrat:
    #  """ Using eigenstrat calculate the first 10 PC's.
    #
    # https://www.hsph.harvard.edu/alkes-price/eigensoft-frequently-asked-questions/
    # however, you can set “familynames: NO” so that only the sample ID name will be used and
    # must meet the 39 character limit.
    # """
#     input:
#     output:
#     shell:
#         "module load eigensoft/4.2; \
#         smartpca.perl -i data/adgc.pruned.3unrelated.shortids.bed \
#         -a data/adgc.pruned.3unrelated.shortids.bim  \
#         -b data/adgc.pruned.3unrelated.shortids.fam  \
#         -o adgc.pruned.3unrelated.pca  \
#         -p adgc.pruned.3unrelated.plot \
#         -e adgc.pruned.3unrelated.eval  \
#         -l adgc.pruned.3unrelated.log   \
#         -m 0 -k 10"

# rule eigenstrat:
# """Must convert long ID's to temporary shortened ID's to make eigenstrat happy.. Eww
# Create update-ids file using awk, file usingUse plink update-ids command with format:
# old_ID1    old_ID2 new_ID1 new_ID2
# """
#     input:
        # fam=
#     output:
        # short_id_fam=
#     shell:
        # "awk '{{print $1,$2,"ID1_"NR,"ID2_"NR}}' adgc.pruned.3unrelated.fam; \
        # plink --fam {input.fam} --update-ids update_long_ids.txt --make-just-fam {output.short_id_fam}"

rule king_robust:
    """ Run King Robust with 3rd degree relative cutoff
    Cutoff value to distinguish between PO and FS is set at IBS0=0.0065
    Relationship summary (total relatives: 0 by pedigree, 3514 by inference)
      Source        MZ      PO      FS      2nd     3rd     OTHER
      ===========================================================
      Inference     1801    166     898     395     254     0

    Families are clustered into 641 new families
    A list of 28730 unrelated individuals saved in file adgc_unrelated_3_newtryunrelated.txt

      3 adgc_hrc_kingallsegs.txt
   5178 adgc_hrc_kingunrelated_toberemoved.txt
  33698 adgc_hrc_kingunrelated.txt

    Notes on the program:
        - Says LD pruning is not recommended. Seems to imply keep as many good
        QC'ed variants as possible.

    /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5
    """
    input:
        final_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "short_names", "adgc_short_names.bed")
    output:
        king_output=expand(os.path.join(PROCESS_DATA_SOURCE, "king", "adgc_hrc_king{ext}"), ext=KING_EXT)
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "king", "adgc_hrc_king"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5 \
        -b {input.final_pruned_bfiles} --unrelated \
        --degree {params.related_cutoff} --plink \
        --prefix {params.king_output_base}"

rule remap_fids_for_king:
    """ Update sample information
        --update-ids [filename]
        These update sample IDs, parental codes, and sexes, respectively. --update-parents now also updates founder/nonfounder status in the current run when appropriate.
        --update-ids expects input with the following four fields:
            Old family ID
            Old within-family ID
            New family ID
            New within-family ID
    """
    input:
        expand(os.path.join(PROCESS_DATA_SOURCE, "qced", "adgc_hrc_merged_common_qc_snps.{ext}"), ext=PLINK_EXT)
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "short_names", "adgc_short_names.{ext}"), ext=PLINK_EXT)
    params:
        fam=os.path.join(PROCESS_DATA_SOURCE, "qced", "adgc_hrc_merged_common_qc_snps.fam"),
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "qced", "adgc_hrc_merged_common_qc_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "short_names", "adgc_short_names"),
        update_id_file=os.path.join(PROCESS_DATA_SOURCE, "short_names", "short_name_update.txt")
    shell:
        "awk '{{print($1, $2, \"IID_\"NR, \"FID_\"NR)}}' {params.fam} > {params.update_id_file} && \
        plink --bfile {params.input_plink_base} \
        --update-ids {params.update_id_file} \
        --make-bed --out {params.output_plink_base}"

rule plink_final_prune:
    """ Takes the prune.in file (maf, geno, indep filters) and creates the final plink set.
    Uses plink 1.9
    Previously used following params
    Options in effect:
            --noweb
            --bfile adgc_combined_common_snps
            --maf 0.01
            --geno 0.02
            --indep-pairwise 1500 150 0.2
            --out adgc.combined.commonsnps.thinned
    """
    input:
        common_snps_bfile=expand(os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_snps.{ext}"), ext=PLINK_EXT)
    output:
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "qced", "adgc_hrc_merged_common_qc_snps.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "qced", "adgc_hrc_merged_common_qc_snps")
    shell:
        "plink --bfile {params.input_plink_base} \
        --extract {params.input_plink_base}.prune.in \
        --make-bed --out {params.output_plink_base}"

rule plink_extract_common_snps_and_filter:
    """ Extracts subset of common SNP's from plink files and then immediately filters and prunes them.
    Plink outputs a file called prune.in/prune.out as a result of the maf, geno, indep filters. Must do
    another plink call to extract the prune.in variants.

    Uses plink 1.9
    """
    input:
        bed=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bed"),
        fam=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.fam"),
        bim=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bim"),
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt")
    output:
        common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_snps.{ext}"), ext=PLINK_EXT)
    params:
        plink_base=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_snps"),
        maf=0.01,
        geno=0.02,
        indep_pairwise="1500 150 0.2"
    shell:
        "plink --bed  {input.bed} \
        --fam {input.fam} \
        --bim {input.bim} \
        --extract {input.common_snps} \
        --maf {params.maf} \
        --geno {params.geno} \
        --indep-pairwise {params.indep_pairwise} \
        --make-bed --out {params.plink_base}"

rule get_common_snps:
    """ Using a simple python script intersect all input files. Input files
    are expected to be a snp id per line."""
    input:
        # snps=expand(os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim"), study=wildcards.study) # Would love to be able to do this..
        snps=lambda wildcards: [os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{}.bim".format(bim)) for bim in BIM_NAMES]
    output:
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt")
    run:
        setlist = []
        for file in input:
            print("Working on file: {}".format(file))
            setlist.append(set(line.strip() for line in open(file, 'r')))
        common_snps = set.intersection(*setlist)
        print(output.common_snps)
        with open(output.common_snps, "w") as f:
            for snp in common_snps:
                f.write(snp + "\n")

rule bim_to_snp_ids_only:
    """ awks out snp ids from bim files and write to new file.
    """
    input:
        bim=os.path.join(BIM_DATA_SOURCE, "{study}.bim")
    output:
        snp=os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim")
    shell:
        "awk '{{print $2}}' {input.bim} > {output.snp}"
