""" ADGC 2.0 - Relatedness analysis

 General overview of process
    1. Given raw genotyped snps for each study, create a list of common snps.
    2. LD-prune list of common snps.
    3. Extract these subset of snps from plink files.
    4. Run King-Robust on this LD-pruned, common dataset to generate list of related (within 3rd degree) individuals.
    6. Remove related individuals leaving one
    7. Run Eigenstrat on this LD-pruned, common, unrelated dataset to generate PC's
    8. Merge PC's with sample covariate files.


    snakemake -np --verbose -j 999 --cluster-config ../scripts/relatedness/cluster.json -s ../scripts/relatedness/Snakefile --cluster \
    "sbatch \
    --ntasks {cluster.ntasks} \
    --time {cluster.time} \
    --mem {cluster.mem} \
    --job-name {cluster.name} \
    --mail-type {cluster.mail-type} \
    --mail-user {cluster.mail-user} \
    --parsable" \
    --cluster-status ../scripts/slurm_status.py
"""

from pathlib import Path
from python_scripts import utils

# This is the shared folder location, however due to lack of space cant be used.
# ROOT = "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/"
ROOT = "/fslhome/fslcollab192/compute"
WORKING_DIR = "ADGC_HRC_COMBINE"
WORKFLOW_NAME = "relatedness_workflow_2nd"

######## Covariate variables
COVAR_DATA_SOURCE = os.path.join(ROOT, "ADGC_Data", "ADGC_Covariates")
COVARIATE_PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", "covariate_workflow")
RELATEDNESS_PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", "relatedness_workflow")
######### End covariate variables

BIM_DATA_SOURCE = os.path.join(ROOT, "ADGC_Data", "ADGC_Raw", "bims")
PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", WORKFLOW_NAME)

FINAL_DATA_SOURCE = os.path.join("/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu", "final")
EXTRA_FOLDER = os.path.join(FINAL_DATA_SOURCE, "auxiliary")

LOGS = os.path.join(PROCESS_DATA_SOURCE, "logs")
PLINK_EXT = ["bed", "bim", "fam"]
KING1_EXT = ["TMP.ped", "TMP.dat", "unrelated.txt", "updateids.txt"]
KING2_EXT = ["allsegs.txt", "unrelated_toberemoved.txt", "unrelated.txt"]
CHROMOSOME = list(range(1,23))

localrules: bim_to_snp_ids_only, get_common_snps, get_all_genotyped_snps, remap_fids_for_king, copy_qc_common_snps_to_extra, prune_unrelated_short_ids, convert_king_unrelated_set_ids_back, prepare_eigen_start_inputs

BIM_NAMES = [Path(name).stem for name in os.listdir(BIM_DATA_SOURCE)]

rule all:
    """
    This workflow assumes the Snakemake in scripts/ was run which is the main
    merging and qc workflow.

    Final output of relatedness workflow is:
            1. Unrelated plink files
            2. Unrelated vcf files
            3. Combined covariate file with new PC's attached
    """
    input:
        os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt"),
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king2", "adgc_hrc_king{ext}"), ext=KING2_EXT),
        plink_covar=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.covar"), # Covar output
        unrelated_plink_covar=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.covar") # Covar output
        # Real outputs
        # expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_king{ext}"), ext=KING1_EXT)
        # expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.{ext}"), ext=PLINK_EXT),
        # expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz.tbi"), chr=CHROMOSOME),
        # os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.par"),
        # os.path.join(EXTRA_FOLDER,  "adgc_hrc_merged_common_qc_snps.bim")


rule unrelated_filter_plink:
    """ Use the list of unrelateds to prune plink."""
    input:
        plink_files=expand(os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.{ext}"), ext=PLINK_EXT),
        unrelated_ids=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_unrelated_ids_plink.txt")
    output:
        expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.{ext}"), ext=PLINK_EXT)
    params:
        plink_input_base=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced"),
        plink_output_base=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated")
    shell:
        "plink --bfile {params.plink_input_base} --keep {input.unrelated_ids} --make-bed --out {params.plink_output_base}"

########### Filter vcf ##########################
rule tabix_vcf:
    """ tabix vcf files"""
    input:
        vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz")
    output:
        output_tbi=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz.tbi")
    shell:
        "tabix -p vcf {input.vcf}"

rule unrelated_filter_vcf:
    """ Use vcftools to prune vcfs of relateds."""
    input:
        full_vcf=os.path.join(FINAL_DATA_SOURCE, "vcf", "adgc_hrc_combined_chr{chr}.vcf.gz"),
        unrelated_ids=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_unrelated_ids_vcf.txt")
    output:
        unrelated_vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz")
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/vcftools_0.1.17/vcftools \
         --keep {input.unrelated_ids} --gzvcf {input.full_vcf} --recode --stdout | bgzip -c > {output.unrelated_vcf}"

########### Done filter vcf ##########################

rule eigenstrat:
    """Must rename plink files, and also recode missing to -100.0
    04:30:00 runtime. Comment out plink or king, depending on what we end up
    using.
    """
    input:
        bed=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_unrelated.bed"),
        pedind=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedind"),
        pedsnp=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedsnp")
    output:
        pca=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.par"),
        eval=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.evec")
    params:
        pca=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca"),
        eval=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.eval"),
        plot=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.plot"),
        number_pca=10
    log:
        os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.log")
    shell:
        "module load eigensoft/4.2; \
        /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/EIG-7.2.1/bin/smartpca.perl \
        -i {input.bed} -a {input.pedsnp} \
        -b {input.pedind} -o {params.pca} \
        -p {params.plot} -e {params.eval} \
        -l {log} -m 0 -k {params.number_pca}"

rule prepare_eigen_start_inputs:
    """ eigen needs the fam and bim files slightly modified. """
    input:
        fam=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_unrelated.fam"),
        bim=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_unrelated.bim")
    output:
        pedind=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedind"),
        pedsnp=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedsnp")
    shell:
        "cp {input.bim} {output.pedsnp}; \
        awk '{{if ($6==-9) print $1,$2,$3,$4,$5,-100.0; else print $0}}' {input.fam} > {output.pedind}"

############################ Pruned unrelated set complete ####################

rule convert_king_unrelated_set_ids_back:
    """ the king unrelated file is in shortened ids. We must now look up these
    id's and get them back to the long ones.
    ADC1_NACC548317_08AD7682___NACC548317_08AD7682 NACC548317_08AD7682 FID_1 IID_1
    """
    input:
        king_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king1", "adgc_hrc_kingunrelated.txt"),
        id_map=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        unrelated_ids_plink=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_unrelated_ids_plink.txt"),
        unrelated_ids_vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_unrelated_ids_vcf.txt")
    shell:
        "utils_script.py create-remap-file {input.king_unrelated} {input.id_map} {output.unrelated_ids_plink} {output.unrelated_ids_vcf}"

# Use rule if using King Robust to filter
rule prune_unrelated_short_ids:
    """ Use list of king unrelated samples as input to a --keep plink command.
    NOTICE: Make sure King removes all the samples!
    """
    input:
        king_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king1", "adgc_hrc_kingunrelated.txt"),
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames.{ext}"), ext=PLINK_EXT)
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king1", "adgc_hrc_unrelated.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king1", "adgc_hrc_unrelated")
    shell:
        "plink --bfile {params.input_plink_base} --keep {input.king_unrelated} --make-bed --out {params.output_plink_base}"

rule plink_rel_cutoff:
    """ Just trying to do the rel cutoff using plink 1.9. Seems to get similiar
    (few less) samples at the end. Save a step of using plink to extract a list
    of unrelated samples.
    """
    input:
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames.{ext}"), ext=PLINK_EXT)
    output:
        output=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning","plink", "adgc_hrc_rel_cutoff.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink", "adgc_hrc_rel_cutoff"),
        related_cutoff=0.125
    shell:
        "plink --bfile {params.input_plink_base} \
        --rel-cutoff {params.related_cutoff} \
        --make-bed --out {params.output_plink_base}"

rule king_robust2:
    """ Run KingRobust 2.1.5 with 3rd degree relative cutoff, but it only
    ever does 2nd degree ;/
    """
    input:
        final_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames.bed")
    output:
        king_output=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king2", "adgc_hrc_king{ext}"), ext=KING2_EXT)
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "king2", "adgc_hrc_king"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5 \
        -b {input.final_pruned_bfiles} --unrelated \
        --degree {params.related_cutoff} \
        --prefix {params.king_output_base}"

rule king_robust1:
    """ Run KingRobust 1.4 with 3rd degree relative cutoff
    /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5
    """
    input:
        final_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames.bed")
    output:
        king_output=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_king{ext}"), ext=KING1_EXT)
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","king1", "adgc_hrc_king"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king1.4 \
        -b {input.final_pruned_bfiles} --unrelated \
        --degree {params.related_cutoff} \
        --prefix {params.king_output_base}"

rule shorten_ids:
    """ Take list of unrelated samples from previous steps and keep them, and
    renames samples for eigenstrat.
    1. Run plink to keep unrelated.
    2. Rename to shorter ID's
    """
    input:
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps.{ext}"), ext=PLINK_EXT)
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames.{ext}"), ext=PLINK_EXT),
        update_id_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    params:
        input_fam=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps.fam"),
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "adgc_hrc_shortnames"),
    shell:
        "awk '{{print($1, $2, \"F\"NR, \"I\"NR)}}' {params.input_fam} > {output.update_id_file} && \
        plink --bfile {params.input_plink_base} \
        --update-ids {output.update_id_file} \
        --make-bed --out {params.output_plink_base}"

############### Use all genotyped snps not just common ###########
# rule plink_extract_all_snps_and_filter:
#     """ Extracts subset of common SNP's from plink files and then immediately filters and prunes them.
#     Plink outputs a file called prune.in/prune.out as a result of the maf, geno, indep filters. Must do
#     another plink call to extract the prune.in variants.
#
#     Uses plink 1.9
#     """
#     input:
#         bed=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bed"),
#         fam=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.fam"),
#         bim=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bim"),
#         all_genotyped_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_all_genotyped_snps.txt")
#     output:
#         common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps.{ext}"), ext=PLINK_EXT)
#     params:
#         plink_base=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps"),
#         maf=0.01,
#         geno=0.02,
#     shell:
#         "plink --bed  {input.bed} \
#         --fam {input.fam} \
#         --bim {input.bim} \
#         --extract {input.all_genotyped_snps} \
#         --maf {params.maf} \
#         --geno {params.geno} \
#         --make-bed --out {params.plink_base}"

# rule get_all_genotyped_snps:
#     """ King Robust suggests using all available snps not just common ones. """
#     input:
#         snps=lambda wildcards: [os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{}.bim".format(bim)) for bim in BIM_NAMES]
#     output:
#         all_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_all_genotyped_snps.txt")
#     shell:
#         "cat {input.snps} | grep ^rs | sort | uniq >> {output.all_snps}"

############### End Use all genotyped snps not just common ###########


rule copy_qc_common_snps_to_extra:
    """ Simply copies the bim from previous step to final folder."""
    input:
        os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps.bim")
    output:
        os.path.join(EXTRA_FOLDER,  "adgc_hrc_merged_common_qc_snps.bim")
    shell:
        "cp {input} {output}"

rule plink_extract_common_snps_and_filter:
    """ Extracts subset of common SNP's from plink files and then immediately filters and prunes them.
    Plink outputs a file called prune.in/prune.out as a result of the maf, geno, indep filters. Must do
    another plink call to extract the prune.in variants.

    Uses plink 1.9
    """
    input:
        bed=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bed"),
        fam=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.fam"),
        bim=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.bim"),
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt")
    output:
        common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps.{ext}"), ext=PLINK_EXT)
    params:
        plink_base=os.path.join(PROCESS_DATA_SOURCE, "adgc_hrc_merged_common_qc_snps"),
        maf=0.01,
        geno=0.02,
    shell:
        "plink --bed  {input.bed} \
        --fam {input.fam} \
        --bim {input.bim} \
        --extract {input.common_snps} \
        --maf {params.maf} \
        --geno {params.geno} \
        --make-bed --out {params.plink_base}"

rule get_common_snps:
    """ Using a simple python script intersect all input files. Input files
    are expected to be a snp id per line."""
    input:
        # snps=expand(os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim"), study=wildcards.study) # Would love to be able to do this..
        snps=lambda wildcards: [os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{}.bim".format(bim)) for bim in BIM_NAMES]
    output:
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt")
    run:
        setlist = []
        for file in input:
            print("Working on file: {}".format(file))
            setlist.append(set(line.strip() for line in open(file, 'r')))
        common_snps = set.intersection(*setlist)
        print(output.common_snps)
        with open(output.common_snps, "w") as f:
            for snp in common_snps:
                f.write(snp + "\n")

rule bim_to_snp_ids_only:
    """ awks out snp ids from bim files and write to new file.
    """
    input:
        bim=os.path.join(BIM_DATA_SOURCE, "{study}.bim")
    output:
        snp=os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim")
    shell:
        "awk '{{print $2}}' {input.bim} > {output.snp}"


###########################
#### Covariate ############
###########################
############## Covariate File preparation ##################################

rule create_covar_subsets_for_final_datasets:
    """ fgreps out subsets of the covariate file for the datasets."""
    input:
        full_covar=os.path.join(EXTRA_FOLDER, "adgc_hrc_full_merged_with_pcs_covar.txt"),
        plink_fam=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.fam"),
        unrelated_plink_fam=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.fam"),
    output:
        plink_covar=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.covar"),
        unrelated_plink_covar=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.covar"),
        tmp_full=temp("full.txt"),
        tmp_unrelated=temp("unrelated.txt")
    shell:
        "rm -f {output.plink_covar}; rm -f {output.unrelated_plink_covar}; \
        awk '{{print $1,$2}}' {input.plink_fam} >> {output.tmp_full}; \
        head -n 1 {input.full_covar} >> {output.plink_covar}; \
        LC_ALL=C fgrep -f {output.tmp_full} {input.full_covar} >> {output.plink_covar}; \
        awk '{{print $1,$2}}' {input.unrelated_plink_fam} >> {output.tmp_unrelated}; \
        head -n 1 {input.full_covar} >> {output.unrelated_plink_covar}; \
        LC_ALL=C fgrep -f {output.tmp_unrelated} {input.full_covar} >> {output.unrelated_plink_covar}"

rule add_pc_to_covar:
    """ Combines all Covariate files """
    input:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar_renamed.txt"),
        eigen=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.evec"),
        short_id_map=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        final_covar=os.path.join(EXTRA_FOLDER, "adgc_hrc_full_merged_with_pcs_covar.txt")
    shell:
        "cat_covar.py add-pcs {input.combined_covar} {input.eigen} {input.short_id_map} {output.final_covar}"

rule update_ids:
    """ Simply updates ids to match our schema"""
    input:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar.txt"),
        short_id_map=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        renamed_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar_renamed.txt")
    shell:
        "cat_covar.py update-ids {input.combined_covar} {input.short_id_map} {output.renamed_covar}"

rule combine_covariate_files:
    """ Combines all Covariate files """
    input:
        COVAR_DATA_SOURCE
    output:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar.txt")
    shell:
        "cat_covar.py combine-covars {input} {output.combined_covar}"
