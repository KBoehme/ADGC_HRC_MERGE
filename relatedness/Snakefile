""" ADGC 2.0 - Relatedness analysis

 General overview of process
    1. Given raw genotyped snps for each study, create a list of common snps.
    2. LD-prune list of common snps.
    3. Extract these subset of snps from plink files.
    4. Run King-Robust on this LD-pruned, common dataset to generate list of related (within 3rd degree) individuals.
    6. Remove related individuals leaving one
    7. Run Eigenstrat on this LD-pruned, common, unrelated dataset to generate PC's
    8. Merge PC's with sample covariate files.


    snakemake -np --verbose -j 999 --cluster-config ../scripts/relatedness/cluster.json -s ../scripts/relatedness/Snakefile --cluster \
    "sbatch \
    --ntasks {cluster.ntasks} \
    --time {cluster.time} \
    --mem {cluster.mem} \
    --job-name {cluster.name} \
    --mail-type {cluster.mail-type} \
    --mail-user {cluster.mail-user} \
    --parsable" \
    --cluster-status ../scripts/slurm_status.py
"""

from pathlib import Path
from python_scripts import utils

# This is the shared folder location, however due to lack of space cant be used.
# ROOT = "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/"
ROOT = "/fslhome/fslcollab192/compute"
WORKING_DIR = "ADGC_HRC_COMBINE"
WORKFLOW_NAME = "relatedness_workflow"

######## Covariate variables
COVAR_DATA_SOURCE = os.path.join(ROOT, "ADGC_Data", "ADGC_Covariates")
COVARIATE_PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", "covariate_workflow")
RELATEDNESS_PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", "relatedness_workflow")
######### End covariate variables

BIM_DATA_SOURCE = os.path.join(ROOT, "ADGC_Data", "ADGC_Raw", "bims")
PROCESS_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "process", WORKFLOW_NAME)

# FINAL_DATA_SOURCE = os.path.join("/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu", "final")
FINAL_DATA_SOURCE = os.path.join(ROOT, WORKING_DIR, "final")
EXTRA_FOLDER = os.path.join(FINAL_DATA_SOURCE, "auxiliary")

LOGS = os.path.join(PROCESS_DATA_SOURCE, "logs")
PLINK_EXT = ["bed", "bim", "fam"]
# KING1_EXT = ["TMP.ped", "TMP.dat", "unrelated.txt", "updateids.txt"]
KING1_KINSHIP_EXT = ["TMP.ped", "TMP.dat", ".kin", ".kin0"]

KING2_EXT = ["allsegs.txt", "unrelated_toberemoved.txt", "unrelated.txt"]
CHROMOSOME = list(range(1,23))

localrules: prune_mostly_unrelated, remove_related_using_kinship, plink_keep_within_status_unrelated, create_shorten_ids, get_list_of_cases, bim_to_snp_ids_only, get_common_snps, get_all_genotyped_snps, remap_fids_for_king, copy_qc_common_snps_to_extra, prune_unrelated_short_ids, create_remap_id_file, prepare_eigen_start_inputs,  create_covar_subsets_for_final_datasets add_pc_to_covar update_ids combine_covariate_files

BIM_NAMES = [Path(name).stem for name in os.listdir(BIM_DATA_SOURCE)]

rule all:
    """
    This workflow assumes the Snakemake in scripts/ was run which is the main
    merging and qc workflow.

    Final output of relatedness workflow is:
            1. Unrelated plink files
            2. Unrelated vcf files
            3. Combined covariate file with new PC's attached
    """
    input:
        expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.{ext}"), ext=PLINK_EXT),
        expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz.tbi"), chr=CHROMOSOME),
        os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.par"),
        os.path.join(EXTRA_FOLDER,  "adgc_hrc_merged_common_qc_snps.bim")


rule unrelated_filter_plink:
    """ Use the list of unrelateds to prune plink."""
    input:
        plink_files=expand(os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.{ext}"), ext=PLINK_EXT),
        unrelated_ids=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_unrelated_ids_plink.txt")
    output:
        expand(os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.{ext}"), ext=PLINK_EXT)
    params:
        plink_input_base=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced"),
        plink_output_base=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated")
    shell:
        "plink --bfile {params.plink_input_base} --keep {input.unrelated_ids} --make-bed --out {params.plink_output_base}"

########### Filter vcf ##########################
rule tabix_vcf:
    """ tabix vcf files"""
    input:
        vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz")
    output:
        output_tbi=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz.tbi")
    shell:
        "tabix -p vcf {input.vcf}"

rule unrelated_filter_vcf:
    """ Use vcftools to prune vcfs of relateds."""
    input:
        full_vcf=os.path.join(FINAL_DATA_SOURCE, "vcf", "adgc_hrc_combined_chr{chr}.vcf.gz"),
        unrelated_ids=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_unrelated_ids_vcf.txt")
    output:
        unrelated_vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_combined_unrelated_chr{chr}.vcf.gz")
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/vcftools_0.1.17/vcftools \
         --keep {input.unrelated_ids} --gzvcf {input.full_vcf} --recode --stdout | bgzip -c > {output.unrelated_vcf}"

########### Done filter vcf ##########################

rule eigenstrat:
    """Must rename plink files, and also recode missing to -100.0
    04:30:00 runtime. Comment out plink or king, depending on what we end up
    using.
    """
    input:
        bed=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen_input", "adgc_hrc_unrelated.bed"),
        pedind=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedind"),
        pedsnp=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedsnp")
    output:
        pca=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.par"),
        eval=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.evec")
    params:
        pca=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca"),
        eval=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.eval"),
        plot=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.plot"),
        number_pca=10
    log:
        os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.log")
    shell:
        "module load eigensoft/4.2; \
        /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/EIG-7.2.1/bin/smartpca.perl \
        -i {input.bed} -a {input.pedsnp} \
        -b {input.pedind} -o {params.pca} \
        -p {params.plot} -e {params.eval} \
        -l {log} -m 0 -k {params.number_pca}"

rule prepare_eigen_start_inputs:
    """ Eigen needs the fam and bim files slightly modified. """
    input:
        fam=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen_input", "adgc_hrc_unrelated.fam"),
        bim=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen_input", "adgc_hrc_unrelated.bim")
    output:
        pedind=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedind"),
        pedsnp=os.path.join(PROCESS_DATA_SOURCE, "related_pruning","eigen", "adgc_hrc_unrelated.pedsnp")
    shell:
        "cp {input.bim} {output.pedsnp}; \
        awk '{{if ($6==-9) print $1,$2,$3,$4,$5,-100.0; else print $0}}' {input.fam} > {output.pedind}"

############################ Pruned unrelated set complete ####################

rule create_remap_id_file:
    """ The king unrelated file is in shortened ids. We must now look up these
    id's and get them back to the long ones for filtering using plink and vcftools
    """
    input:
        final_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_kingunrelated.txt"),
        id_map=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        unrelated_ids_plink=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_unrelated_ids_plink.txt"),
        unrelated_ids_vcf=os.path.join(FINAL_DATA_SOURCE, "unrelated", "vcf", "adgc_hrc_unrelated_ids_vcf.txt")
    shell:
        "utils_script.py create-remap-file {input.final_unrelated} {input.id_map} {output.unrelated_ids_plink} {output.unrelated_ids_vcf}"

# Use rule if using King Robust to filter
rule prune_unrelated_short_ids:
    """ Use list of king unrelated samples as input to a --keep plink command.
    NOTICE: Make sure King removes all the samples!
    """
    input:
        final_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_kingunrelated.txt"),
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT)
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_unrelated.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_unrelated")
    shell:
        "plink --bfile {params.input_plink_base} --keep {input.final_unrelated} --make-bed --out {params.output_plink_base}"

################### Squeeze as many cases ###############

rule final_king_unrelated:
    """ Run KingRobust 1.4 with 3rd degree relative cutoff
    /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5
    """
    input:
        mostly_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "mostly_unrelated", "adgc_hrc_unrelated.bed")
    output:
        king_output=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_kingunrelated.txt")
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "eigen_input", "adgc_hrc_king"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king1.4 \
        -b {input.mostly_pruned_bfiles} --unrelated \
        --degree {params.related_cutoff} \
        --prefix {params.king_output_base}"

rule intermediate_plink_filter:
    """ Use list of king unrelated samples as input to a --keep plink command.
    NOTICE: Make sure King removes all the samples!
    """
    input:
        mostly_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "mostly_unrelated_set.txt"),
        final_pruned_bfiles=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT)
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "mostly_unrelated", "adgc_hrc_unrelated.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "mostly_unrelated", "adgc_hrc_unrelated")
    shell:
        "plink --bfile {params.input_plink_base} --keep {input.mostly_unrelated} --make-bed --out {params.output_plink_base}"

rule remove_related_using_kinship:
    """ Given full fam file, kinship estimates for said fam file set, and a
    mapping for id to status filter out related individuals attempting to
    maximize cases > controls > missing. """
    input:
        unrelated_cases=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "king1", "adgc_hrc_kingunrelated.txt"),
        fam_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps.fam"),
        kinship_estimates=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "full_kinship", "remove_het", "adgc_hrc_kinship.kin0"),
        status_map=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_id_status_map.txt"),
    output:
        mostly_unrelated=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "mostly_unrelated_set.txt"),
    shell:
        "utils_script.py filter-related \
        {input.unrelated_cases} \
        {input.fam_file} \
        {input.status_map} \
        {input.kinship_estimates} \
        {output.mostly_unrelated}"

rule king1_remove_het_kinship:
    """ Calculate kinship for the whole set"""
    input:
        final_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps.bed")
    output:
        king_output=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "full_kinship", "remove_het", "adgc_hrc_kinship.kin0")
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "full_kinship", "remove_het", "adgc_hrc_kinship"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king1.4 \
        -b {input.final_pruned_bfiles} --kinship \
        --degree {params.related_cutoff} \
        --prefix {params.king_output_base}"

################### End Squeeze as many cases ###############

rule king_robust1_cases:
    """ Run KingRobust 1.4 with 3rd degree relative cutoff
    /fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king2.1.5
    """
    input:
        final_pruned_bfiles=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "adgc_cases_only.bed")
    output:
        king_output=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "king1", "adgc_hrc_kingunrelated.txt")
    params:
        king_output_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "king1", "adgc_hrc_king"),
        related_cutoff=3
    shell:
        "/fslhome/fslcollab192/fsl_groups/fslg_KauweLab/compute/ADGC_2018_combined/alois.med.upenn.edu/programs/king1.4 \
        -b {input.final_pruned_bfiles} --unrelated \
        --degree {params.related_cutoff} \
        --prefix {params.king_output_base}"

rule plink_keep_cases:
    """ Just get cases for King Robust run.
    Uses plink 1.9
    """
    input:
        plink_files=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT),
        case_ids=os.path.join(PROCESS_DATA_SOURCE, "case_ids.txt")
    output:
        common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "adgc_cases_only.{ext}"), ext=PLINK_EXT)
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps"),
        plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "cases", "adgc_cases_only"),
    shell:
        "plink --bfile {params.input_plink_base} \
        --keep {input.case_ids} \
        --make-bed --out {params.plink_base}"

# rule copy_qc_common_snps_to_extra:
#     """ Simply copies the bim from previous step to final folder."""
#     input:
#         os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.bim")
#     output:
#         os.path.join(EXTRA_FOLDER,  "adgc_hrc_merged_common_qc_snps.bim")
#     shell:
#         "cp {input} {output}"

rule filter_hets:
    input:
        common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT),
        het_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.het"),
    output:
        output_plink=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT),
        het_filters=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps.het"),
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "het_filter", "adgc_hrc_common_qc_snps"),
    shell:
        "awk '{{if($6 > 0.1 || $6 < -0.1) print $1,$2}}' {input.het_file} > {output.het_filters} && \
        plink --bfile {params.input_plink_base} \
        --remove {output.het_filters} \
        --make-bed --out {params.output_plink_base}"

rule plink_extract_common_snps_and_filter:
    """ Extracts subset of common SNP's from plink files and then immediately filters and prunes them.
    Plink outputs a file called prune.in/prune.out as a result of the maf, geno, indep filters. Must do
    another plink call to extract the prune.in variants.

    Uses plink 1.9
    """
    input:
        plink_files=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink", "adgc_hrc_full_shortnames.{ext}"), ext=PLINK_EXT),
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt"),
    output:
        common_extracted=expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.{ext}"), ext=PLINK_EXT),
        het_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps.het"),
        copy_bim=os.path.join(EXTRA_FOLDER,  "adgc_hrc_merged_common_qc_snps.bim")
    params:
        input_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink", "adgc_hrc_full_shortnames"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink_qced", "adgc_hrc_common_qc_snps"),
        maf=0.01,
        geno=0.02,
    shell:
        "plink --bfile {params.input_plink_base} \
        --extract {input.common_snps} \
        --maf {params.maf} \
        --geno {params.geno} \
        --het \
        --make-bed --out {params.output_plink_base} && \
        cp {params.output_plink_base}.bim {output.copy_bim}"

rule get_list_of_cases:
    """ From the status map, get a list of cases and a list of controls+missing"""
    input:
        status_map=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_id_status_map.txt")
    output:
        case_ids=os.path.join(PROCESS_DATA_SOURCE, "case_ids.txt"),
    shell:
        "awk '{{if($3 == 2) print $1,$2}}' {input.status_map} > {output.case_ids} "

rule full_plink_shorten_ids:
    """ Simply update ID's for full plink dataset, so we dont have to keep mapping
    back and forth.
    """
    input:
        final_plink=expand(os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.{ext}"), ext=PLINK_EXT),
        update_id_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt"),
    output:
        expand(os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink", "adgc_hrc_full_shortnames.{ext}"), ext=PLINK_EXT),
    params:
        input_plink_base=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced"),
        output_plink_base=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "plink", "adgc_hrc_full_shortnames"),
    shell:
        "plink --bfile {params.input_plink_base} \
        --update-ids {input.update_id_file} \
        --make-bed --out {params.output_plink_base}"

rule create_shorten_ids:
    input:
        input_covar=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.covar"),
    output:
        update_id_file=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt"),
        short_id_status_mapping=os.path.join(PROCESS_DATA_SOURCE, "related_pruning", "short_id_status_map.txt"),
    shell:
        "awk 'FNR > 1 {{print($1, $2, \"F\"NR, \"I\"NR)}}' {input.input_covar} > {output.update_id_file} && \
        awk 'FNR > 1 {{print(\"F\"NR, \"I\"NR, $9)}}' {input.input_covar} > {output.short_id_status_mapping}"

rule get_common_snps:
    """ Using a simple python script intersect all input files. Input files
    are expected to be a snp id per line."""
    input:
        # snps=expand(os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim"), study=wildcards.study) # Would love to be able to do this..
        snps=lambda wildcards: [os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{}.bim".format(bim)) for bim in BIM_NAMES]
    output:
        common_snps=os.path.join(EXTRA_FOLDER, "adgc_hrc_common_snps.txt")
    run:
        setlist = []
        for file in input:
            print("Working on file: {}".format(file))
            setlist.append(set(line.strip() for line in open(file, 'r')))
        common_snps = set.intersection(*setlist)
        print(output.common_snps)
        with open(output.common_snps, "w") as f:
            for snp in common_snps:
                f.write(snp + "\n")

rule bim_to_snp_ids_only:
    """ awks out snp ids from bim files and write to new file.
    """
    input:
        bim=os.path.join(BIM_DATA_SOURCE, "{study}.bim")
    output:
        snp=os.path.join(PROCESS_DATA_SOURCE, "bim_snps", "{study}.bim")
    shell:
        "awk '{{print $2}}' {input.bim} > {output.snp}"


###########################
#### Covariate ############
###########################

rule create_covar_subsets_for_final_datasets:
    """ fgreps out subsets of the covariate file for the datasets."""
    input:
        full_covar=os.path.join(EXTRA_FOLDER, "adgc_hrc_full_merged_with_pcs_covar.txt"),
        plink_fam=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.fam"),
        unrelated_plink_fam=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.fam"),
    output:
        plink_covar=os.path.join(FINAL_DATA_SOURCE, "plink", "adgc_hrc_merged_qced.covar"),
        unrelated_plink_covar=os.path.join(FINAL_DATA_SOURCE, "unrelated", "plink", "adgc_hrc_merged_unrelated.covar"),
        tmp_full=temp("full.txt"),
        tmp_unrelated=temp("unrelated.txt")
    shell:
        "rm -f {output.plink_covar}; rm -f {output.unrelated_plink_covar}; \
        awk '{{print $1,$2}}' {input.plink_fam} >> {output.tmp_full}; \
        head -n 1 {input.full_covar} >> {output.plink_covar}; \
        LC_ALL=C fgrep -f {output.tmp_full} {input.full_covar} >> {output.plink_covar}; \
        awk '{{print $1,$2}}' {input.unrelated_plink_fam} >> {output.tmp_unrelated}; \
        head -n 1 {input.full_covar} >> {output.unrelated_plink_covar}; \
        LC_ALL=C fgrep -f {output.tmp_unrelated} {input.full_covar} >> {output.unrelated_plink_covar}"

rule add_pc_to_covar:
    """ Combines all Covariate files """
    input:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar_renamed.txt"),
        eigen=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "eigen", "adgc_pruned_3unrelated.pca.evec"),
        short_id_map=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        final_covar=os.path.join(EXTRA_FOLDER, "adgc_hrc_full_merged_with_pcs_covar.txt")
    shell:
        "cat_covar.py add-pcs {input.combined_covar} {input.eigen} {input.short_id_map} {output.final_covar}"

rule update_ids:
    """ Simply updates ids to match our schema"""
    input:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar.txt"),
        short_id_map=os.path.join(RELATEDNESS_PROCESS_DATA_SOURCE, "related_pruning", "short_name_update.txt")
    output:
        renamed_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar_renamed.txt")
    shell:
        "cat_covar.py update-ids {input.combined_covar} {input.short_id_map} {output.renamed_covar}"

rule combine_covariate_files:
    """ Combines all Covariate files """
    input:
        COVAR_DATA_SOURCE
    output:
        combined_covar=os.path.join(PROCESS_DATA_SOURCE, "adgc_initial_merged_covar.txt")
    shell:
        "cat_covar.py combine-covars {input} {output.combined_covar}"
